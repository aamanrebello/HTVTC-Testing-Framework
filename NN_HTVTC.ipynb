{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN-HTVTC.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Dense Neural Network\n",
        "\n",
        "Begin by importing the necessary libraries. Seeds are set for reproducibility."
      ],
      "metadata": {
        "id": "o0wEE8dzsg3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.random.seed(1234)\n",
        "from keras.models import Sequential\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(123)\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "j_DOsbftskh_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and preprocess the MNIST data set."
      ],
      "metadata": {
        "id": "7wBTA12ts554"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "# Load and split data\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "#Transform 3D X data into 2D \n",
        "X_train_flatten = X_train.reshape(X_train.shape[0], X_train.shape[1] * X_train.shape[2])\n",
        "X_test_flatten = X_test.reshape(X_test.shape[0], X_test.shape[1] * X_test.shape[2])\n",
        "\n",
        "#Normalise X data elements to all be in range [0,1]\n",
        "X_train_flatten = X_train_flatten.astype('float32')\n",
        "X_test_flatten = X_test_flatten.astype('float32')\n",
        "X_train_flatten /= 255\n",
        "X_test_flatten /= 255\n",
        "\n",
        "#Adjust y data so that numerical categorical labels become one-hot vectors of size 10\n",
        "Y_train_class = np_utils.to_categorical(y_train, 10)\n",
        "Y_test_class = np_utils.to_categorical(y_test, 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUUZo3vbs8jl",
        "outputId": "ddee85bc-976b-48d5-82b3-8e96d7e154e3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a function to generate a 3-layer dense neural network, with parameterisable structure, to perform the prediction task."
      ],
      "metadata": {
        "id": "1yO_lETJuG84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM = 784\n",
        "\n",
        "def create_network(L1_neurons, L2_neurons, **kwargs):\n",
        "  L1_activation = 'relu'\n",
        "  L2_activation = 'relu'\n",
        "  L3_activation = 'softmax'\n",
        "\n",
        "  L3_neurons = 10\n",
        "\n",
        "  if 'L1_activation' in kwargs.keys():\n",
        "    L1_activation = kwargs['L1_activation']\n",
        "  if 'L2_activation' in kwargs.keys():\n",
        "    L2_activation = kwargs['L2_activation']\n",
        "  if 'L3_activation' in kwargs.keys():\n",
        "    L3_activation = kwargs['L3_activation']\n",
        "  if 'L3_neurons' in kwargs.keys():\n",
        "    L3_neurons = kwargs['L3_neurons']\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Dense(L1_neurons, activation=L1_activation, input_shape=(INPUT_DIM,), name='L1'))\n",
        "  model.add(Dense(L2_neurons, activation=L2_activation, name='L2'))\n",
        "  model.add(Dense(L3_neurons, activation=L3_activation, name='L3'))\n",
        "\n",
        "  return model\n",
        "\n",
        "test_model = create_network(64, 32, L3_neurons = 25)\n",
        "test_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEdb6NSnMgNB",
        "outputId": "6514e0ab-ec6d-49b1-9b57-a697e1499d24"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " L1 (Dense)                  (None, 64)                50240     \n",
            "                                                                 \n",
            " L2 (Dense)                  (None, 32)                2080      \n",
            "                                                                 \n",
            " L3 (Dense)                  (None, 25)                825       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 53,145\n",
            "Trainable params: 53,145\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a function which will accept the hyperparameters, generate the neural network and evaluate it against the data set."
      ],
      "metadata": {
        "id": "K6z_ol7IPeC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(L1_neurons, L2_neurons, **kwargs):\n",
        "  model = create_network(L1_neurons, L2_neurons, **kwargs)\n",
        "\n",
        "  loss_function = 'categorical_crossentropy'\n",
        "  optimizer = 'adam'\n",
        "  if 'loss_function' in kwargs.keys():\n",
        "    loss_function = kwargs['loss_function']\n",
        "  if 'optimizer' in kwargs.keys():\n",
        "    optimizer = kwargs['optimizer']\n",
        "\n",
        "  model.compile(loss=loss_function, optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "  batch_size = 32\n",
        "  if 'batch_size' in kwargs.keys():\n",
        "    batch_size = kwargs['batch_size']\n",
        "\n",
        "  history = None\n",
        "  if 'epochs' in kwargs.keys():\n",
        "    epochs = kwargs['epochs']\n",
        "    history = model.fit(X_train_flatten, Y_train_class, batch_size=batch_size, epochs=epochs, verbose=0)\n",
        "  else:\n",
        "    early_stopping = EarlyStopping(monitor='loss', patience=5)\n",
        "    history = model.fit(X_train_flatten, Y_train_class, epochs=1000, batch_size=batch_size, callbacks=[early_stopping], verbose=0)\n",
        "\n",
        "  score = model.evaluate(X_test_flatten, Y_test_class, verbose=0)\n",
        "\n",
        "  return {\n",
        "      'loss': score[0],\n",
        "      'accuracy': score[1]\n",
        "  }\n",
        "\n",
        "evaluate(64, 32, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNS6chEaP40S",
        "outputId": "8a4c9459-4f40-4f89-f846-aa133f2ef32e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.9728000164031982, 'loss': 0.10417211055755615}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the tensor completion tests for speed and max memory."
      ],
      "metadata": {
        "id": "bTgKhi0vEXVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#IMPORTS==============================================\n",
        "import unittest\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "from generateerrortensor import generateIncompleteErrorTensor\n",
        "from trainmodels import evaluationFunctionGenerator, crossValidationFunctionGenerator\n",
        "from loaddata import loadData, trainTestSplit, extractZeroOneClasses, convertZeroOne\n",
        "from tensorcompletion import tensorcomplete_TMac_TT\n",
        "from tensorcompletion import ket_augmentation, inverse_ket_augmentation\n",
        "from tensorsearch import findBestValues, hyperparametersFromIndices\n",
        "import regressionmetrics\n",
        "import classificationmetrics\n",
        "\n",
        "quantity = 'CPU-TIME'\n",
        "\n",
        "known_fraction = 0.25\n",
        "\n",
        "#Load dataset\n",
        "task = 'classification'\n",
        "data = loadData(source='sklearn', identifier='breast_cancer', task=task)\n",
        "data_split = trainTestSplit(data, method = 'cross_validation')\n",
        "\n",
        "budget_type = 'features'\n",
        "budget_fraction = 0.5\n",
        "func = crossValidationFunctionGenerator(data_split, algorithm='svm-rbf', task=task, budget_type=budget_type, budget_fraction=budget_fraction)\n",
        "\n",
        "#Start timer/memory profiler/CPU timer\n",
        "start_time = None\n",
        "if quantity == 'EXEC-TIME':\n",
        "    import time\n",
        "    start_time = time.perf_counter_ns()\n",
        "elif quantity == 'CPU-TIME':\n",
        "    import time\n",
        "    start_time = time.process_time_ns()\n",
        "elif quantity == 'MAX-MEMORY':\n",
        "    import tracemalloc\n",
        "    tracemalloc.start()\n",
        "\n",
        "#Generate range dictionary\n",
        "ranges_dict = {\n",
        "    'L1_neurons': {\n",
        "        'start': 0.05,\n",
        "        'end': 5.00,\n",
        "        'interval': 0.05,\n",
        "        },\n",
        "    'L2_neurons': {\n",
        "        'start': 0.05,\n",
        "        'end': 5.00,\n",
        "        'interval': 0.05,\n",
        "        },\n",
        "    'L1_activation': ['relu'],\n",
        "    'L2_activation': ['relu'],\n",
        "    'optimizer': ['adam'],\n",
        "    'batch_size':\n",
        "    }\n",
        "\n",
        "#Generate incomplete tensor\n",
        "incomplete_tensor, known_indices = generateIncompleteErrorTensor(func, ranges_dict, known_fraction, metric=classificationmetrics.indicatorFunction, eval_trials=1)\n",
        "print('TENSOR GENERATED')\n",
        "\n",
        "expected_rank = [1]\n",
        "#Apply tensor completion\n",
        "predicted_tensor, _, _ = tensorcomplete_TMac_TT(incomplete_tensor, known_indices, expected_rank, convergence_tolerance=1e-15, iteration_limit=100000)\n",
        "\n",
        "#Find best hyperparameter value\n",
        "result = findBestValues(predicted_tensor, smallest=True, number_of_values=1)\n",
        "values, indices = result['values'], result['indices']\n",
        "hyperparameter_values = hyperparametersFromIndices(indices, ranges_dict)\n",
        "\n",
        "#End timer/memory profiler/CPU timer\n",
        "result = None\n",
        "if quantity == 'EXEC-TIME':\n",
        "    end_time = time.perf_counter_ns()\n",
        "    result = end_time - start_time\n",
        "elif quantity == 'CPU-TIME':\n",
        "    end_time = time.process_time_ns()\n",
        "    result = end_time - start_time\n",
        "elif quantity == 'MAX-MEMORY':\n",
        "    _, result = tracemalloc.get_traced_memory()\n",
        "    tracemalloc.stop()\n",
        "\n",
        "#Recreate cross-validation generator\n",
        "data_split = trainTestSplit(data, method = 'cross_validation')\n",
        "#Find the true loss for the selcted combination\n",
        "truefunc = crossValidationFunctionGenerator(data_split, algorithm='svm-rbf', task=task)\n",
        "true_value = truefunc(metric=classificationmetrics.indicatorFunction, **hyperparameter_values[0])\n",
        "\n",
        "print(f'hyperparameters: {hyperparameter_values}')\n",
        "print(f'function values: {values}')\n",
        "print(f'True value: {true_value}')\n",
        "print(f'{quantity}: {result}')"
      ],
      "metadata": {
        "id": "dsZkV5tbEc8l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}